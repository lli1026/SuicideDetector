{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac6350f5-ffbd-41da-b93b-3aea9b6a8175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68a4a1ed-de3b-44de-bb96-f79ea24b6f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None) #display the full text otherwise the text will be truncated\n",
    "data = pd.read_csv('Suicide_Detection.csv', index_col = [0]) # assign the first column as index\n",
    "df = data.sample(n=2000, random_state=42) # sampling 50000 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dc2eb8f-9e32-449d-9ebc-aa8598c01e96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# encode the labels\n",
    "# Define a mapping for unique values\n",
    "value_mapping = {'suicide': 0, 'non-suicide': 1}\n",
    "# Use the map function to apply the mapping to the column\n",
    "df['class'] = df['class'].map(value_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4ac806f-145c-46dd-8615-382c46a7f91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['class'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2218393-04ef-4e62-afe2-66033a09bba9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = X_train.tolist()\n",
    "X_val = X_val.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "088634dc-111a-46d6-9f44-3686ce5d20f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_data = tokenizer(X_train, return_tensors=\"np\", padding=True,truncation=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = np.array(y_train)  # Label is already an array of 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67830ab3-7d35-4ff9-bc43-9cffe53b2a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_data_val = tokenizer(X_val, return_tensors=\"np\", padding=True,truncation=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data_val = dict(tokenized_data_val)\n",
    "\n",
    "labels_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185163d5-b8af-40c2-890b-9f3c1c0b51df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[  101,  1790,   112, ...,     0,     0,     0],\n",
       "        [  101,  1247,   112, ...,     0,     0,     0],\n",
       "        [  101,  1139,  4153, ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  101,   146,  1276, ...,     0,     0,     0],\n",
       "        [  101,   146,  1198, ...,  7299,  1143,   102],\n",
       "        [  101, 10259, 23926, ...,     0,     0,     0]]),\n",
       " 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 1, 1, 1],\n",
       "        [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7df8d896-dbff-435f-b06f-2cf43a2d99bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9c8088b-be4f-417d-af19-ac6f02a3645e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1971250-8c94-4224-8137-17cef718666c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1600\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_data),len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3aa287ac-db0a-4a70-8f1b-0b846ff52485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b139ba61-9d45-4bfd-9dc3-2245b6bb9bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower learning rates are often better for fine-tuning transformers\n",
    "model.compile(optimizer=Adam(3e-5),metrics='accuracy')  # No loss argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c8f37-7486-490e-a0cc-36a46dcad560",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath='model_checkpoint.h5', save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92dfd5-330f-416e-8d01-2fda2a77de7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 9495s 191s/step - loss: 0.3652 - accuracy: 0.8300 - val_loss: 0.2432 - val_accuracy: 0.9125\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 7562s 152s/step - loss: 0.1425 - accuracy: 0.9606 - val_loss: 0.2186 - val_accuracy: 0.9275\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 11397s 230s/step - loss: 0.0628 - accuracy: 0.9825 - val_loss: 0.1875 - val_accuracy: 0.9375\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 58733s 1063s/step - loss: 0.0323 - accuracy: 0.9919 - val_loss: 0.2695 - val_accuracy: 0.9325\n",
      "Epoch 5/5\n",
      "10/50 [=====>........................] - ETA: 1:55:09 - loss: 0.0225 - accuracy: 0.9937"
     ]
    }
   ],
   "source": [
    "hist = model.fit(tokenized_data, labels, validation_data=(tokenized_data_val,labels_val),epochs=5, callbacks=checkpoint_callback)\n",
    "# Kernal restarting issue on home PC, works for iMAC, very costy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e16a3-d5d1-41b5-9684-a42ca0132701",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(10,3))\n",
    "ax[0].plot(hist.history['loss'],color='teal',label='loss')\n",
    "ax[0].plot(hist.history['val_loss'],color='orange',label='val_loss')\n",
    "ax[0].set_title('Loss',fontsize=20)\n",
    "ax[0].legend(loc='lower left')\n",
    "\n",
    "ax[1].plot(hist.history['categorical_accuracy'],color='teal',label='categorical_accuracy')\n",
    "ax[1].plot(hist.history['val_categorical_accuracy'],color='orange',label='val_categorical_accuracy')\n",
    "ax[1].set_title('Accuracy',fontsize=20)\n",
    "ax[1].legend(loc='lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
